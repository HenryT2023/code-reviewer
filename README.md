# CodeReviewer

AI-powered project evaluation system with multi-role perspectives. Supports **static**, **dynamic**, and **UI** evaluation modes for any type of software project â€” from single-repo apps to large monorepos.

## Features

### Multi-Role Evaluation

9 built-in roles with customizable prompts:

| Role | Focus |
|------|-------|
| ğŸ‘” **Boss** | ROI, market positioning, risk, scalability |
| ğŸª **Merchant** | UX, feature fit, competitive analysis, value perception |
| âš™ï¸ **Operator** | Efficiency, reporting, stability, exception handling |
| ğŸ—ï¸ **Architect** | Code organization, API design, data model, security, DevOps |
| ğŸ¨ **Artist** | Visual design, UX aesthetics, interaction patterns |
| ğŸ“ˆ **Growth** | User acquisition, retention, monetization strategy |
| ğŸ’° **Pricing** | Pricing model, revenue optimization, market fit |
| ğŸ“Š **Data** | Data architecture, analytics, metrics design |
| ğŸ”´ **RedTeam** | Security vulnerabilities, attack vectors, risk assessment |
| ğŸ¯ **UserInterview** | Real user perspective with 8 Enneagram-based personas |

### Evaluation Types

| Type | Description |
|------|-------------|
| **Static** | Code structure, metrics, API/DB detection (default) |
| **Dynamic** | Server startup, health check, API endpoint testing |
| **UI** | Playwright-based UI flow testing with screenshots |
| **Full** | Static + Dynamic + UI combined |

### Evaluation Modes

- **Standard**: Individual role evaluations
- **Launch-Ready**: Multi-role debate + orchestrator synthesis for go/no-go decisions

### Role Self-Evolution

- **Reflection**: AI self-critique after each evaluation, identifying blind spots and prompt improvements
- **Synthesis**: Aggregate reflections to generate new roles and improve existing prompts
- **Rerun API**: Backfill reflections for historical evaluations

### Evolution Closed Loop (NEW)

- **MREP (Measurable, Referenceable, Evidence-based, Precise)**: Extract structured claims from role outputs with file/line references, then programmatically verify against actual codebase
- **Grounded Judge**: AI judge scores evaluation quality using external knowledge-based checklists (coverage 40%, accuracy 25%, calibration 20%, specificity 15%)
- **Prompt Override Layer**: Per-project prompt overrides from synthesis results, with version history and rollback support
- **A/B Comparison**: Automated prompt effectiveness validation â€” runs baseline vs variant evaluations, compares judge scores, auto-applies if improvement exceeds threshold

### Static Analysis

- API endpoint detection (Express, FastAPI, Django, Flask, Next.js, etc.)
- Database entity & ORM detection (SQLAlchemy, TypeORM, Prisma, Sequelize, etc.)
- Monorepo / multi-service recognition with per-service breakdown
- Code quality metrics: complexity, language distribution, largest files, test coverage
- Python engineering quality (pyproject.toml, Ruff, Black, MyPy, Alembic)

### Other Features

- **Custom Role Prompts**: Override default system prompts per role
- **Real-time Progress**: WebSocket-based status updates
- **Persistent Storage**: JSON-based evaluation history
- **Report Export**: Markdown and JSON export formats
- **Trend Analysis**: Track project improvement over time

## Tech Stack

| Layer | Technology |
|-------|------------|
| Backend | Node.js, Express, TypeScript |
| Frontend | React 18, Vite, Ant Design 5 |
| AI Models | DeepSeek V3 (default), Alibaba Qwen, OpenAI GPT-4 |
| UI Testing | Playwright |
| Charts | ECharts |
| Storage | JSON file-based persistence |

## Quick Start

### Prerequisites

- Node.js 18+
- npm or yarn

### Installation

```bash
# Clone repository
git clone https://github.com/HenryT2023/code-reviewer.git
cd code-reviewer

# Install root dependencies (concurrently)
npm install

# Install server & web dependencies
cd server && npm install
cd ../web && npm install

# Configure environment
cp .env.example .env
# Edit .env with your API keys
```

### Running

```bash
# Option 1: Start both services concurrently
npm run dev

# Option 2: Start separately
# Terminal 1 â€” Backend
cd server && npm run dev

# Terminal 2 â€” Frontend
cd web && npm run dev
```

### Access

| Service | URL |
|---------|-----|
| Frontend | http://localhost:9001 |
| Backend API | http://localhost:9000 |
| WebSocket | ws://localhost:9000/ws |

## API

### Core Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/evaluate` | Start project evaluation |
| GET | `/api/evaluate/:id` | Get evaluation result |
| GET | `/api/history` | List evaluation history |
| GET | `/api/health` | Health check |

### Evolution Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/evolution/reflections` | List all reflections |
| GET | `/api/evolution/reflections/:id` | Get reflection for evaluation |
| POST | `/api/evolution/rerun-reflection/:id` | Rerun reflection for completed evaluation |
| POST | `/api/evolution/synthesize` | Trigger evolution synthesis |
| GET | `/api/evolution/stats` | Get evolution statistics |
| GET | `/api/evolution/overrides` | List prompt overrides for project |
| POST | `/api/evolution/rollback/:role` | Rollback prompt override |

### MREP Endpoints

| Method | Endpoint | Description |
| ------ | -------- | ----------- |
| GET | `/api/mrep/:evaluationId` | Get MREP reports for evaluation |
| POST | `/api/mrep/:evaluationId/verify` | Verify MREP claims against codebase |
| GET | `/api/mrep/stats/aggregate` | Get aggregate MREP statistics |

### Judge Endpoints

| Method | Endpoint | Description |
| ------ | -------- | ----------- |
| GET | `/api/judge/:evaluationId` | Get grounded judgment for evaluation |
| GET | `/api/judge/stats` | Get judge statistics and trends |
| POST | `/api/judge/:evaluationId/rerun` | Rerun judgment for evaluation |

### A/B Test Endpoints

| Method | Endpoint | Description |
| ------ | -------- | ----------- |
| POST | `/api/ab-test/trigger` | Start A/B test for synthesis |
| GET | `/api/ab-test/:id` | Get A/B test status and result |
| GET | `/api/ab-test` | List A/B tests |
| POST | `/api/ab-test/:id/apply` | Manually apply A/B test result |

### Export Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/export/markdown/:id` | Export report as Markdown |
| GET | `/api/export/json/:id` | Export report as JSON |
| GET | `/api/trends/stats` | Get overall statistics |

### Evaluate Request Body

```jsonc
{
  "projectPath": "/path/to/project",
  "projectName": "My Project",
  "roles": ["boss", "merchant", "operator", "architect", "artist"],
  "context": "Brief project description for AI context",
  "depth": "deep",                    // "quick" | "deep"
  "mode": "launch-ready",             // "standard" | "launch-ready"
  "evaluationType": "full",           // "static" | "dynamic" | "ui" | "full"
  "rolePrompts": {                    // optional: override default prompts
    "architect": "Custom system prompt for architect role..."
  }
}
```

## Project Structure

```
code-reviewer/
â”œâ”€â”€ server/                  # Backend (Express + TypeScript)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ ai/              # AI model integration
â”‚       â”‚   â”œâ”€â”€ qwen.ts      #   DeepSeek/Qwen API client
â”‚       â”‚   â”œâ”€â”€ roles.ts     #   Role definitions & prompts
â”‚       â”‚   â”œâ”€â”€ orchestrator.ts  # Debate & orchestrator synthesis
â”‚       â”‚   â””â”€â”€ role-evolution.ts # Reflection & evolution synthesis
â”‚       â”œâ”€â”€ analyzers/       # Static analysis modules
â”‚       â”‚   â”œâ”€â”€ api.ts       #   API endpoint detection
â”‚       â”‚   â”œâ”€â”€ database.ts  #   Database & ORM detection
â”‚       â”‚   â”œâ”€â”€ metrics.ts   #   Code metrics & complexity
â”‚       â”‚   â”œâ”€â”€ quality.ts   #   Engineering quality checks
â”‚       â”‚   â””â”€â”€ structure.ts #   Project structure & monorepo
â”‚       â”œâ”€â”€ eval/            # Dynamic & UI evaluation
â”‚       â”‚   â”œâ”€â”€ runtime.ts   #   Server startup & API testing
â”‚       â”‚   â”œâ”€â”€ ui.ts        #   Playwright UI flow testing
â”‚       â”‚   â””â”€â”€ types.ts     #   Evaluation type definitions
â”‚       â”œâ”€â”€ db/              # JSON file persistence
â”‚       â”œâ”€â”€ grounded-judge/  # Grounded Judge module
â”‚       â”‚   â”œâ”€â”€ types.ts     #   Judge types & dimensions
â”‚       â”‚   â”œâ”€â”€ reference-builder.ts  # Tech stack checklist builder
â”‚       â”‚   â””â”€â”€ judge.ts     #   Judge scoring logic
â”‚       â”œâ”€â”€ mrep/            # MREP extraction & verification
â”‚       â”‚   â”œâ”€â”€ parser.ts    #   Claim extraction from role output
â”‚       â”‚   â”œâ”€â”€ verifier.ts  #   Programmatic claim verification
â”‚       â”‚   â””â”€â”€ metrics.ts   #   MREP quality metrics
â”‚       â”œâ”€â”€ prompt-overrides/ # Prompt override layer
â”‚       â”‚   â””â”€â”€ manager.ts   #   Override CRUD & rollback
â”‚       â”œâ”€â”€ routes/          # Express routes
â”‚       â”‚   â”œâ”€â”€ evaluate.ts  #   Evaluation API
â”‚       â”‚   â”œâ”€â”€ evolution.ts #   Evolution API
â”‚       â”‚   â”œâ”€â”€ judge.ts     #   Judge API
â”‚       â”‚   â”œâ”€â”€ mrep.ts      #   MREP API
â”‚       â”‚   â””â”€â”€ ab-test.ts   #   A/B Test API
â”‚       â””â”€â”€ ws/              # WebSocket progress events
â”œâ”€â”€ web/                     # Frontend (React + Vite + Ant Design)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ pages/           # Evaluate, Report, History, Dashboard
â”‚       â””â”€â”€ services/        # API client
â”œâ”€â”€ .env.example
â””â”€â”€ package.json             # Root scripts (concurrently)
```

## Environment Variables

```env
# AI Model API Keys (at least one required)
DEEPSEEK_API_KEY=your-deepseek-api-key    # Recommended (default)
DASHSCOPE_API_KEY=your-qwen-api-key
OPENAI_API_KEY=your-openai-api-key

# Server Port
PORT=9000
```

## License

MIT
